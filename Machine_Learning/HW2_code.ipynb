{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "# import pandas\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-05860aea0e96>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-05860aea0e96>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data_tr=pandas.read_csv(\"\"/Users/fanerror/GitHub/DataFest2018/wdaycount.csv\",header=0)\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "data_tr=pandas.read_csv(\"\"/Users/fanerror/GitHub/DataFest2018/wdaycount.csv\",header=0)\n",
    "print (data_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "csvFile = open(\"/Users/fanerror/GitHub/DataFest2018/wdaycount.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "data_tr = list()\n",
    "data_tr_lab = list()\n",
    "for item in reader:\n",
    "    # omit first line\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    data_tr.append(item[1])\n",
    "    data_tr_lab.append(item[0])\n",
    "csvFile.close()\n",
    "\n",
    "# load test data\n",
    "csvFile = open(\"./hw2data_1/reviews_te.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "\n",
    "data_te = list()\n",
    "data_te_lab = list()\n",
    "for item in reader:\n",
    "    # omit first line\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    data_te.append(item[1])\n",
    "    data_te_lab.append(item[0])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320122, 1000000)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_te),len(data_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first time here food tastes great good environment choice between mimi s or here is a hard one yet there is wifi here families galore i didn t bring my son yet i see lots of families eating here for what you get you could go to sunset station yet the service was fast and prompt when i am in the area it s now a good place to fill in the answer to what do you want to eat\n"
     ]
    }
   ],
   "source": [
    "print(data_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram representation\n",
    "def Unigram(d):\n",
    "    text = d.split(' ')\n",
    "    wordcount = {}\n",
    "    for item in text:\n",
    "        if item not in wordcount.keys():\n",
    "            wordcount[item] = 1\n",
    "        else:\n",
    "            wordcount[item] += 1\n",
    "    return wordcount\n",
    "\n",
    "# Term frequency-inverse document frequency (tf-idf) weighting\n",
    "def tf_idf(d, D):\n",
    "    \n",
    "    ### define idf function\n",
    "    def idf(t,D):\n",
    "        count = 0\n",
    "        for document in D:\n",
    "            text = document.split(' ')\n",
    "            if t in text:\n",
    "                count += 1\n",
    "        return len(D)/count\n",
    "    \n",
    "    wordcount = {}\n",
    "    text = d.split(' ')\n",
    "    for item in text:\n",
    "        if item not in wordcount.keys():\n",
    "            wordcount[item] = 1\n",
    "        else:\n",
    "            wordcount[item] += 1\n",
    "\n",
    "    for key in wordcount.keys():\n",
    "        idf_key = idf(key,D)\n",
    "        wordcount[key] = wordcount[key]*np.log10(idf_key)\n",
    "    return wordcount\n",
    "\n",
    "# Bigram representation.\n",
    "def Bigram(d):\n",
    "    text = d.split(' ')\n",
    "    wordcount = {}\n",
    "    for iteration in range(len(text)-1):\n",
    "        left = text[iteration]\n",
    "        right = text[iteration+1]\n",
    "        bigram = (left,right)\n",
    "        if bigram not in wordcount.keys():\n",
    "            wordcount[bigram] = 1\n",
    "        else:\n",
    "            wordcount[bigram] += 1\n",
    "    return wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define operators\n",
    "## because not every data point have same feature\n",
    "def dict_plus(x,y,oper=1):\n",
    "    '''\n",
    "    x,y are both dictionary\n",
    "    '''\n",
    "    if oper == 0:\n",
    "        oper = -1\n",
    "    z = {}\n",
    "    for key in set(x.keys()) | set(y.keys()):\n",
    "        if key in set(x.keys()) & set(y.keys()):\n",
    "            z[key] = x[key] + oper*y[key]\n",
    "        elif key not in x.keys():\n",
    "            z[key] = y[key]\n",
    "        elif key not in y.keys():\n",
    "            z[key] = x[key]\n",
    "    return(z)\n",
    "\n",
    "def dict_mult(x,y):\n",
    "    '''\n",
    "    x,y are both dictionary\n",
    "    '''\n",
    "    z = {}\n",
    "    for key in set(x.keys()) & set(y.keys()):\n",
    "        z[key] = x[key] * y[key]\n",
    "    return(sum(z.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online(data,label):\n",
    "    '''\n",
    "    data: training data, list of dictionaries\n",
    "        dictionary, key is feature and value is wordcount\n",
    "    label: label corresponding to training data\n",
    "    '''\n",
    "    \n",
    "    # Initialize w\n",
    "    w = [{}]\n",
    "    \n",
    "    # create index\n",
    "    index = range(len(data))\n",
    "    # shuffle index\n",
    "    random.shuffle(index)\n",
    "    \n",
    "    for i in index:\n",
    "        sign_value = dict_mult(data[i],w[-1])\n",
    "        if sign_value*label[i] <= 0:\n",
    "            w.append(dict_plus(w[-1],data[i],label[i]))\n",
    "    \n",
    "    print(\"second shuffle\")\n",
    "    # second shuffle index\n",
    "    random.shuffle(index)\n",
    "    for i in index:\n",
    "        sign_value = dict_mult(data[i],w[-1])\n",
    "        if sign_value*label[i] <= 0:\n",
    "            w.append(dict_plus(w[-1],data[i],label[i]))\n",
    "        else:\n",
    "            w.append(w[-1])\n",
    "            \n",
    "    return(w)\n",
    "\n",
    "def online_perceptron(data,label,representations=1):\n",
    "    '''\n",
    "    data: raw training data, list\n",
    "    label: raw label data, list\n",
    "    representations:1 - Unigram representation\n",
    "                    2 - Term frequency-inverse document frequency \n",
    "                    3 - Bigram representation\n",
    "                    \n",
    "    '''\n",
    "    \n",
    "    # convert raw data to list of dictionaries\n",
    "    train_list= []\n",
    "    train_label = []\n",
    "    for i in range(len(data)):\n",
    "        if representations == 1:\n",
    "            train_list.append(Unigram(data[i]))\n",
    "        elif representations == 3:\n",
    "            train_list.append(Bigram(data[i]))\n",
    "        elif representations == 2:\n",
    "            train_list.append(tf_idf(data[i],data))\n",
    "        if label[i]==1:\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(-1)\n",
    "    w = online(train_list,train_label)\n",
    "    w_final = {}\n",
    "    for wi in w:\n",
    "        w_final = dict_plus(wi,w_final)\n",
    "    for key in w_final.keys():\n",
    "        w_final[key] = w_final[key] / (len(data) + 1)\n",
    "    return (w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testmodel(data,label,w,representations=1):\n",
    "    '''\n",
    "    data: raw test data, list\n",
    "    label: raw label data, list\n",
    "    representations:1 - Unigram representation\n",
    "                    2 - Term frequency-inverse document frequency \n",
    "                    3 - Bigram representation\n",
    "                    \n",
    "    '''\n",
    "    \n",
    "    # convert raw data to list of dictionaries\n",
    "    test_list= []\n",
    "    test_label = []\n",
    "    for i in range(len(data)):\n",
    "        if representations == 1:\n",
    "            test_list.append(Unigram(data[i]))\n",
    "        elif representations == 3:\n",
    "            test_list.append(Bigram(data[i]))\n",
    "        elif representations == 2:\n",
    "            test_list.append(tf_idf(data[i],data))\n",
    "        if label[i]==1:\n",
    "            test_label.append(1)\n",
    "        else:\n",
    "            test_label.append(-1)\n",
    "    count = 0        \n",
    "    for i in range(len(test_list)):\n",
    "        predict = dict_mult(w,test_list[i])\n",
    "        if predict * test_label[i] < 0:\n",
    "            count += 1\n",
    "    print count\n",
    "    print len(data)\n",
    "    accuracy = format(float(count) / float(len(data)),'.5f')\n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second shuffle\n",
      "1\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "w1 = online_perceptron(data_tr[1:100],data_tr_lab[1:100],2)\n",
    "accu1 = testmodel(data_te[1:100],data_te_lab[1:100],w1,2)\n",
    "# w2 = online_perceptron(data_tr,data_tr_lab,2)\n",
    "# accu2 = testmodel(data_te,data_te_lab,w2,2)\n",
    "# w3 = online_perceptron(data_tr,data_tr_lab,3)\n",
    "# accu3 = testmodel(data_te,data_te_lab,w3,3)\n",
    "# print (accu1,accu2,accu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01010\n"
     ]
    }
   ],
   "source": [
    "print accu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
