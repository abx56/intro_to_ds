---
title: "Project1"
author: "Fan Yang"
date: "01-30-2018"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

# Step 0: check and install needed packages. Load the libraries and functions. 

```{r, message=FALSE, warning=FALSE}
# packages.used=c("rvest", "tibble", "qdap", 
#                 "sentimentr", "gplots", "dplyr",
#                 "tm", "syuzhet", "factoextra", 
#                 "beeswarm", "scales", "RColorBrewer",
#                 "RANN", "tm", "topicmodels")
# 
# # check packages that need to be installed.
# packages.needed=setdiff(packages.used, 
#                         intersect(installed.packages()[,1], 
#                                   packages.used))
# # install additional packages
# if(length(packages.needed)>0){
#   install.packages(packages.needed, dependencies = TRUE)
# }

# load packages
library("rvest")
library("tibble")
# You may need to run
#system("ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib")
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

library(wordcloud)
library(RColorBrewer)
library(tidytext)

library("xlsx")
library("ggplot2")

source("./lib/plotstacked.R")
source("./lib/speechFuncs.R")
```
This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

# Step 1: Data harvest:

First import all data needed: InauguationDates.txt, InaugurationInfo.xlsx and all speeches text.
```{r,warning=FALSE}
inaug.dates <- read.table("./data/InauguationDates.txt",header=TRUE,sep="\t")
inaug.info <- read.xlsx("./data/InaugurationInfo.xlsx",sheetName="Sheet1",header=T,stringsAsFactors = FALSE)
inaug.info$Words <- as.numeric(inaug.info$Words)

filenames = list.files("./data/InauguralSpeeches") #get all file name
dir = paste("./data/InauguralSpeeches/",filenames,sep="")
n = length(dir)
speeches = list()
for (i in 1:n){
  filename = paste("./data/InauguralSpeeches/inaug",inaug.info$File[i],"-",inaug.info$Term[i],".txt",sep="")
  new.data = paste(readLines(filename, n=-1, skipNul=TRUE),collapse=" ")
  speeches = c(speeches,new.data)
}
names(speeches) <- paste(inaug.info$File, inaug.info$Term,sep="-")
```

# Step 2: data Processing --- generate list of sentences

Use "?", ".", "!", "|",";" as stop point of one sentence and extract all sentences from speeches.
```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:58){
  sentences=sent_detect(speeches[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(inaug.info[i,-ncol(inaug.info)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Delete all non-sentences resulted by erroneous extra end-of-sentence marks. 
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 
```

# Step 3: Data analysis
## 3.1. total number of words
Before we analyze length of single sentences, let's have an prewview of total number of words in a speech. Draw a plot of length of speech against their time order.
```{r,warning=FALSE}
ggplot(inaug.info) +
  geom_point(aes(1:58,Words)) +
  geom_smooth(aes(1:58,Words))
```
Notice that as time went by, the total number of words in a speech tend to be less and converges to around 2000. As we know, shorter speech leads to shorter time scale. We can draftly conclude that the speeches tend to be more concise. The speakers knew that the longer their speech is, the less interest people have to hear their words. But too less words can not precisely convey their political thoughts. Therefore, the number of words in a speech have the tendency to be within some certain range.

## 3.2. length of sentences
Now we want to find something about the length of sentences.
```{r, fig.width = 3, fig.height = 3.4}
sentence.list$TimeOrdered=reorder(sentence.list$File,
                                  1:nrow(sentence.list),
                                  order=T)
tail(sentence.list$TimeOrdered,150)
beeswarm(word.count~TimeOrdered,
         data=sentence.list,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.44, cex.lab=0.8,
         spacing=1.2/nlevels(sentence.list$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Inaugural Speeches")
```

As we can see on above plot, y-coordinate follows the time order. GeorgeWashington is the first president while DonaldJTrump is the present president. We found that as time went by, the number of words in a sentence becomes less and less. The presidents tend to use less words in a sentence.

What are these short sentences?
```{r}
sentence.list%>%
  filter(File=="GeorgeWashington", 
         word.count<=10&word.count>1)%>%
  select(sentences)%>%sample_n(2)

sentence.list%>%
  filter(File=="ThomasJefferson", 
         word.count<=5&word.count>1)%>%
  select(sentences)%>%sample_n(5)

sentence.list%>%
  filter(File=="AbrahamLincoln", 
         word.count<=5&word.count>1)%>%
  select(sentences)%>%sample_n(4)

sentence.list%>%
  filter(File=="FranklinDRoosevelt", 
         word.count<=5&word.count>1)%>%
  select(sentences)%>%sample_n(5)

sentence.list%>%
  filter(File=="BarackObama", 
         word.count<=5&word.count>1)%>%
  select(sentences)%>%sample_n(5)

sentence.list%>%
  filter(File=="DonaldJTrump", 
         word.count<=5&word.count>1)%>%
  select(sentences)%>%sample_n(5)
```
Let's select 6 very famous presidents among American history. Each of them represents different time period. George Washington represents the very beginning of American history while DonaldJ Trump stands for the lastest.

From the output above, George Washington used a sentence with less than 10 words only twice while DonaldJ Trump used a sentence with less than 5 words more than 5 times. George Washington and Thomas Jefferson talks more about freedom and the foundation of America. Abraham Lincoln and Franklin D. Roosevelt who were presidents during war period, their worsds fight for peace and encourage people during hard days. As for DonaldJ Trump and Barack Obama, who are presidents during peace period but suffered from economic crisis, their words focuses more on recovery of economy.

As pace of history pushes forward, the American people faced different problems during different period and their presidents focused on the most impressive topic for people's life.

# Step 3: Data analysis
## 3.2. sentiment analsis

### Sentence length variation over the course of the speech, with emotions. 

```{r, fig.height=2.5, fig.width=2}
par(mfrow=c(6,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)

f.plotsent.len(In.list=sentence.list, InFile="GeorgeWashington", 
               President="George Washington")

f.plotsent.len(In.list=sentence.list, InFile="ThomasJefferson", 
               President="Thomas Jefferson")

f.plotsent.len(In.list=sentence.list, InFile="AbrahamLincoln", 
               President="Abraham Lincoln")

f.plotsent.len(In.list=sentence.list, InFile="FranklinDRoosevelt", 
               President="Franklin D. Roosevelt")

f.plotsent.len(In.list=sentence.list, InFile="BarackObama", 
               President="Barack Obama")

f.plotsent.len(In.list=sentence.list, InFile="DonaldJTrump", 
               President="Donald Trump")
```
We can see that the older presidents(George Washington and Thomas Jefferson) tend to use long sentences to express their opinions precisely. While the newer presidents(Barac kObama and Donald Trump) tend to use short sentences to encourage people and convey their feelings to get emotional resonance.

#### What are the emotionally charged sentences?

```{r}
print("George Washington")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="GeorgeWashington", word.count>=4)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Thomas Jefferson")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="ThomasJefferson", word.count>=5)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Abraham Lincoln")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="AbrahamLincoln", word.count>=4)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Franklin D. Roosevelt")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="FranklinDRoosevelt", word.count>=4)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Barack Obama")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="BarackObama", word.count>=4)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Donald Trump")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="DonaldJTrump", word.count>=5)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

```

### Clustering of emotions
```{r, fig.width=2, fig.height=2}

par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```
We can find that a president must convey positive feelings like trust in order to get people's trust.
```{r, fig.height=3.3, fig.width=3.7}
presid.summary=tbl_df(sentence.list)%>%
  filter(File%in%sel.comparison)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
  )

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
              5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

# Step 3: Data analysis
## 3.3. Topic modeling


For topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. 

```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

### Text mining
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

#### Text basic processing

```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

#### Topic modeling

Gengerate document-term matrices. 

```{r}
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

```

Run LDA

```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("./out/LDAGibbs",k,"DocsToTopics.csv"))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("./out/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("./out/LDAGibbs",k,"TopicProbabilities.csv"))
```
```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms[1,]
```
Based on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic. This part require manual setup as the topics are likely to change. 

```{r}
topics.hash=c("Freedom", "great", "will", "work", "shall", "people", "state", "time", "world", "law", "war", "public", "every", "nation", "can")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
orders = as.factor(as.numeric(corpus.list.df$FileOrdered)[nrow(corpus.list.df):1])
corpus.list.df<- cbind(corpus.list.df,orders)
```
## Clustering of topics
```{r, fig.width=3, fig.height=4}
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(orders, Freedom:can)%>%
              group_by(orders)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]

topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 14),
          trace = "none", density.info = "none")


```



#Step 3 - Inspect an overall wordcloud
```{r, fig.height=6, fig.width=6}
dtm.tidy=tidy(dtm)
dtm.overall=summarise(group_by(dtm.tidy, term), sum(count))

wordcloud(dtm.overall$term, dtm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```


#Step 4 - compute TF-IDF weighted document-term matrices for individual speeches. 

```{r}
dtm.doc <- DocumentTermMatrix(docs,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm.doc)
```


#Step 5- Interactive visualize important words in individual speeches
```{r, warning=FALSE}
library(shiny)

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1', speeches, selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches, selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3, min = 20, 
                              max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),

    server = function(input, output, session) {
      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.count1=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.term2=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech2))],
dtm.count2=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech2))])
      })

      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech2)
      })
    },

    options = list(height = 600)
)
```

















